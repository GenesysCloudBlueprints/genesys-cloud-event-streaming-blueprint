---
title: Stream Conversation Details to AWS Kinesis
author: evelyn.hannon
indextype: blueprint
icon: blueprint
image: images/flowchart.png
category: 5
summary: |
  This blueprint outlines steps and advice on how to setup, use and customise the Genesys Cloud integration with AWS Kinesis.  
---

This blueprint outlines steps and advice on how to setup, use and customise the Genesys Cloud integration with AWS Kinesis.

![Streaming Overview](images/flowchart.png)

## Table of Contents

* [Architecture Design Diagram](#architecture-design-diagram "Architecture Design Diagram")
* [Information Shared by default](#information-shared-by-default "Information Shared by default")
* [Setup](#setup "Setup")
* [Troubleshooting](#troubleshooting "Troubleshooting")
* [Definitions](#definitions "Definitions")
* [Documentation](#documentation "Documentation")

## Solution components

* **Genesys Cloud** - A suite of Genesys cloud services for enterprise-grade communications, collaboration, and contact center management. Contact center agents use the Genesys Cloud user interface.
* **AWS SQS** - A distributed message queuing service. It supports programmatic sending of messages via web service applications as a way to communicate over the Internet.
* **Amazon Kinesis** - A family of services provided by Amazon Web Services for processing and analyzing real-time streaming data at a large scale.
* **AWS IAM** - Identity and Access Management that controls access to AWS resources such as services or features.
* **AWS Lambda** - A serverless computing service for running code without creating or maintaining the underlying infrastructure.
* **Amazon EventBridge** - A scalable, serverless event bus that streams real-time data to selected targets based on custom routing rules.

## Pre-requirements

### Specialized knowledge

1. Admin access to Genesys Cloud
2. Admin access to Amazon Web Services
3. Java 11+
4. Apache Maven tools
5. npm
6. jq or base64

### Genesys Cloud account

* A Genesys Cloud CX 1 license. For more information, see [Genesys Cloud Pricing](https://www.genesys.com/pricing "Opens the Genesys Cloud pricing article").
* The Master Admin role in Genesys Cloud. For more information, see [Roles and permissions overview](https://help.mypurecloud.com/?p=24360 "Opens the Roles and permissions overview article") in the Genesys Cloud Resource Center.

## Implementation steps

You can implement Genesys Cloud objects via the UI or by using Terraform.
* [Configure Genesys Cloud and Amazon using Terraform](#configure-genesys-cloud-using-terraform)
* [Configure Genesys Cloud and Amazon manually](#configure-genesys-cloud-and-amazon-manually)

### Information Shared by default

The following table lists the information that is shared by default. 

| Name           | Type    | Description                                                                                                                                                         |
|----------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| conversationId | UUID    | The unique identifier for the conversation.                                                                                                                         |
| customerId     | UUID    | The unique identifier of the customer involved in the conversation. Acquired via the address in CustomerStartEvent.                                                 |
| direction      | String  |                                                                                                                                                                     |
| duration       | Long    | How long the conversation lasted, in milliseconds                                                                                                                   |
| mediaType      | Enum    | The media type of the conversation.                                                                                                                                 |
| provider       | String  |                                                                                                                                                                     |
| sessionId      | UUID    | The unique identifier for the session associated with the conversation.                                                                                             |
| timeStamp      | Long    |                                                                                                                                                                     |
| topicName      | String  | The topic of the conversation - specifically the notification topic of the event.                                                                                   |
| version        | Integer |                                                                                                                                                                     |

### Download the repository containing the project files

1. Clone the [genesys-cloud-event-streaming-blueprint](https://github.com/GenesysCloudBlueprints/genesys-cloud-event-streaming-blueprint "Goes to the genesys-cloud-event-streaming-blueprint") in GitHub.
2. Build the sample code needed for AWS Lambda
  1. Clone the repository with the source code: `git clone <REPO_URL>`
  2. Open directory with the source code in a console: `cd <PROJECT_ROOT>`
  3. Run command `mvn clean verify -U`. It will download dependencies, compile, test and pack everything into a jar file
  4. If no errors happened on the previous steps, the jar file ready to upload to AWS Lambda: `<PROJECT_ROOT>/target/genesys-external-streaming-integration.jar`

## Configure Genesys Cloud and Amazon using Terraform

### Set up Genesys Cloud

1. You need to set the following environment variables in a terminal window before you can run this project using the Terraform provider:

 * `GENESYSCLOUD_OAUTHCLIENT_ID` - This variable is the Genesys Cloud client credential grant Id that CX as Code executes against. 
 * `GENESYSCLOUD_OAUTHCLIENT_SECRET` - This variable is the Genesys Cloud client credential secret that CX as Code executes against. 
 * `GENESYSCLOUD_REGION` - This variable is the Genesys Cloud region in your organization.
 * `AWS_ACCESS_KEY_ID` - The AWS Access Key you must set up in your Amazon account to allow the AWS Terraform provider to act against your account.
 * `AWS_SECRET_ACCESS_KEY` - The AWS Secret you must set up in your Amazon account to allow the AWS Terraform provider to act against your account.

2. Set the environment variables in the folder where Terraform is running. 

### Configure your Terraform build

Set the following values in the **/terraform/dev.auto.tfvars** file, specific to your Genesys Cloud organization:

* `genesys_cloud_organization_id` - Your Genesys Cloud organization ID from **Admin** > **Organization Settings** > **Advanced**.
* `aws_account_id`- The AWS Account ID where the event source will be made.
* `aws_region`- The AWS region where the event source will be made available for an event bus. (e.g. us-east-1)
* `aws_event_bus_name`- A unique name appended to the Event Source in the AWS account. Maximum of 64 characters consisting of lower/upper case letters, numbers, ., -, _.
* `aws_sqs_queue_name`- The AWS SQS queue name
* `aws_stream_name`- The AWS stream name
* `aws_iam_poicy_name`- The AWS IAM policy name
* `aws_iam_user_name`- The AWS IAM user name
* `aws_iam_lambda_role_name`- The AWS IAM lambda role name
* `aws_lambda_function_name`- The AWS lambda function name
* `aws_eventbridge_name`- The AWS EventBridge name

The following is an example of the dev.auto.tfvars file.

```
genesys_cloud_organization_id = "genesys-cloud-org-id"
aws_account_id                = "aws-account-id"
aws_region                    = "aws-region"
aws_event_bus_name            = "gc-event-streaming"
aws_sqs_queue_name            = "gc-event-streaming-queue"
aws_stream_name               = "gc-event-streaming-kinesis"
aws_iam_poicy_name            = "gc-event-streaming-iam-policy"
aws_iam_user_name             = "gc-event-streaming-iam-user"
aws_iam_lambda_role_name      = "gc-event-streaming-iam-lambda"
aws_lambda_function_name      = "gc-event-streaming-lambda"
aws_eventbridge_name          = "gc-event-streaming-eventbridge"
```

### Run Terraform

The blueprint solution is now ready for your organization to use. 

1. Change to the **/terraform** folder and issue the following commands:

* `terraform init` - This command initializes a working directory containing Terraform configuration files.

* `terraform plan` - This command executes a trial run against your Genesys Cloud organization and displays a list of all the Genesys Cloud resources created. Review this list and ensure that you are comfortable with the plan before moving on to the next step.

* `terraform apply --auto-approve` - This command creates and deploys the necessary objects in your Genesys Cloud account. The --auto-approve flag completes the required approval step before the command creates the objects.

Once the `terraform apply --auto-approve` command has completed, you should see the output of the entire run along with the number of objects that Terraform successfully created. The following points should be remembered:

* In this project, assume you are running using a local Terraform backing state. In this case, the `tfstate` files are created in the same folder where the project is running. It is not recommended to use local Terraform backing state files unless you are running from a desktop or are comfortable deleting files.

* As long as you keep your local Terraform backing state projects, you can tear down this blueprint solution by changing to the `docs/terraform` folder. You can also issue a `terraform destroy --auto-approve` command. All objects currently managed by the local Terraform backing state are destroyed by this command.

## Configure Genesys Cloud and Amazon manually

### Genesys Cloud EventBridge Integration

The first step to make the integration between Genesys Cloud and AWS Kinesis work is to set up integration with AWS EventBridge for sending the events. Each time a conversation event is generated, the integration will send the event from Genesys Cloud to the customers AWS account.

![EventBridge Integration Configuration](images/AdminUIEventBridgeSourceConfiguration.png)

1. Login into Genesys Cloud. Go to Admin → Integration, click **+** button  to add an integration, select **Amazon EventBridge Source**. The screen similar to the screenshot above will be opened.
2. **AWS Account ID** and **AWS Account Region** fields can be found by login into AWS console → click link with account name in the top right corner
  ![AWS user summary](images/AWSUserSummary.png)
3. **Event Source Suffix** can be any string (it is better to avoid spaces and special characters). This value will be used as a name of EventBridge bus in AWS, so it should be meaningful and unique per AWS account.
4. **Topic Filtering** are the topics used to stream data from to AWS EventBridge. For conversation events it should be at least `v2.detail.events.conversation.{id}.customer.end` and `v2.detail.events.conversation.{id}.customer.start`. Click [here](/api/rest/v2/notifications/available_topics) of a list of available topics for the integration.
5. If you want to use wrap-up names and other data in your external service, you should include topics relating to the data you want now, as seen in the Available Topics list above.

Steps above are enough but in case of any problems the comprehensive instruction on how to set up the integration can be found [here](https://help.mypurecloud.com/articles/configure-the-amazon-EventBridge-integration/), this is a good starting point for troubleshooting.

#### Testing

Once the integration toggle switched into **Enabled** position, there is no error icon right to it and a toast message appears with confirmation that the integration established successfully. Apart from that, the new event bus is created for AWS EventBridge (see explanation below).

### AWS SQS Dead Letter Queue

If an error happens on conversation event processing, this conversation event should not be lost. It should be stored somewhere, analysed by a developer to identify a root cause of the problem and reprocessed again after the root cause is fixed. This architecture pattern is called **Dead Letter Queue** (DLQ) and it can be implemented in different ways. As a part of this blueprint, implementation based on AWS SQS is suggested.

1. Login AWS web console, find SQS (Simple Queue Service) service
2. Click **Create queue** button
3. Type: Standard (FIFO queues are not supported as fallback destination in AWS Lambda)
4. Visibility timeout: must be longer than timeout of AWS Lambda (see below), use 900 seconds value if you are not sure 
5. Message retention period: 14 days 
6. Leave all other parameters as is

### AWS Kinesis Data Streams

AWS Kinesis Data Stream (further AWS Kinesis or just Kinesis) is a cloud service for streaming huge amounts of events in real time. For more information, click [here](https://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=2&dn=2).

1. Login into AWS web console, open **Kinesis** service.
2. Click **Create Data Stream** button.
3. Enter a name of the data stream, which will be used to read data from your external service. The other parameters can be left default or set according to the desired throughput.

![Amazon Kinesis base page](images/AmazonKinesisBasePage.png)

![Create data stream](images/CreateAmazonKinesisDataStream.png)

### AWS IAM

The last step in AWS configuration is creating users for the external service. To read data from AWS, the external service must have access to particular AWS resources and only to them. This is the minimal list of permissions the external service user should be granted with:

```
  kinesis:GetShardIterator
  kinesis:GetRecords
  kinesis:DescribeStream
  kinesis:ListStreams
```

These permissions allow the user to read from a specific Kinesis stream and get names of all Kinesis streams in the AWS account.

Before creating a user, the following IAM policy must be created. AEP user permissions:

```
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Sid": "stmntServiceReadFromTestKinesisStream",
        "Effect": "Allow",
        "Action": [
          "kinesis:GetShardIterator",
          "kinesis:GetRecords",
          "kinesis:DescribeStream"
        ],
        "Resource": "<KINESIS_ARN>"
      },
      {
        "Sid": "stmntServiceListAllKinesisStreams",
        "Effect": "Allow",
        "Action": "kinesis:ListStreams",
        "Resource": "*"
      }
    ]
  }
```

Using the policy, the user can be created:

1. On the left menu of **AWS IAM** screen, click **Users**, then click **Add users** button
2. Enter a meaningful username unique per account. Select **Access Key - Programmatic access** credentials type
3. Click **Next**
4. On **Permissions** screen, select **Attach existing policies directly** and select newly created policy for access to the Kinesis stream
5. Click **Next**
6. Tags can be skipped
7. Click **Next**
8. Review the parameters and click **Create user** button. As result, Access and Secret keys will be shown on the screen. It is necessary to save them now, they will cannot be retrieved further.

### AWS Lambda

To stream the conversation events from EventBridge to Kinesis, we require a lambda in the middle that will consume the events from EventBridge, process them and send them onto Kinesis - which will handle sending them onwards to your external service.

Included in [our repository](https://github.com/GenesysCloudBlueprints/genesys-cloud-event-streaming-blueprint) is a sample Lambda function. A customer will have to deploy it to their AWS account.

If the customer wishes to build their own, in high level terms they will have to:

1. Consume ScheduledEvents from EventBridge.
2. Check that the event is one of the ADE events to be consumed.
3. Process the event and create a Kinesis record, based off the data that needs to be consumed within the external service.
4. Send the Kinesis record out to the required Kinesis stream.

#### Testing and Verify

AWS Lambda web console page shows number of invocations, how many of them were successful or failed and it is possible to navigate to the related AWS Cloudwatch logs of the Lambda.

In addition, logs will be produced by the lambda. Depending on how your logging systems are set up, these logs should be available to be processed and read elsewhere.

### AWS EventBridge

AWS EventBridge is a cloud service which works as a router for the events. Depending on the event's bus and schema AWS EventBridge can route the event will be routed to the desired destination.

Here is a brief overview of the service: <https://docs.aws.amazon.com/EventBridge/latest/userguide/eb-what-is.html>

1. Login into AWS web console
2. Find **EventBridge** using the search bar on the top. As result, Amazon EventBridge dashboard will be open
3. On the left menu, click **[Partner event sources](https://us-east-1.console.aws.amazon.com/events/home?region=us-east-1#/partners)**
4. Find the event bus with name ending with value of **Event Source Suffix** set in the Genesys Cloud integration (see above).
5. Click **Associate with event bus**.
  **If it does not exist then return to setting up the Genesys Cloud integration as that step was not successful.**
6. Click **Rules** on the left menu, as result **Rules** page is open
7. Click **Create Rule** button
  - Name - any meaningful name of the rule
  - Event bus - the event bus with name ending with value of **Event Source Suffix**
  - Toggle **Enable the rule on the selected event bus** should be left in **Enabled** position
  - Rule type should be used the default value **Rule with an event pattern**
  - Click **Next**
  - Event source - **All events**
  - Sample event - skip
  - Event pattern - this is a filter, events which do not follow the  pattern will be filtered out. It is recommended to skip this value at the beginning and update it if necessary after making sure that the integration works correctly.
  - Click **Next**
  - Now the target has to be configured. AWS EventBridge supports multiple target per bus, it is necessary to add the lambda introduced in one of the previous steps.
  - Click **Next**
  - Tags can be skipped
  - Review the parameters and click **Create** button

#### Testing

Now it is necessary to make sure that the conversation events are flowing from Genesys Cloud to AWS Kinesis.

1. Start and finish a test conversation
2. Login to the AWS account, find Cloudwatch service on the top search bar
3. On the left menu of Cloudwatch page, click **All Metrics** under
4. Search the stream metrics by Kinesis Stream name, select it
5. Click **Graphed metrics** tab, tick **GetRecords.Success** metric. The graph should show non-zero value of the metric. Note that Cloudwatch needs up to 5 minutes to update the metric after the event is put into the stream

## Troubleshooting

### How to verify that the Kinesis stream is getting data?

In order to be able to set up the external service we need to see what the payload it's receiving from Kinesis is. You can do this using the AWS CLI to read from the stream. Here are the example commands for the stream referred to above.

```{ "title":"aws kinesis describe-stream --stream-name genesys-test-external-kinesis-stream --region us-east-1 |jq", "language": "json", "tabsToSpaces": 2, "showLineNumbers": true, "highlight": "3-5,7" }
{
    "Shards": [
        {
            "ShardId": "shardId-000000000000",
            "HashKeyRange": {
            "StartingHashKey": "0",
            "EndingHashKey": "340282366920938463463374607431768211455"
        }
    ]...
}
```

:::primary
**Note:** You will have to choose a starting time close to when you know conversation data was sent or else you will have to iterate over empty gets until you find one.
:::

```{ "title":"aws kinesis get-shard-iterator --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON --stream-name Foo --region us-east-1 --timestamp 2022-03-25T09:35:00.480-00:00 | jq", "language": "json", "tabsToSpaces": 2, "showLineNumbers": true, "highlight": "3-5,7" }
{
    "ShardIterator": "AAAAAAAAAAHSywljv0zEgPX4NyKdZ5wryMzP9yALs8NeKbUjp1IxtZs1Sp+KEd9I6AJ9ZG4lNR1EMi+9Md/nHvtLyxpfhEzYvkTZ4D9DQVz/mBYWRO6OTZRKnW9gd+efGN2aHFdkH1rJl4BL9Wyrk+ghYG22D2T1Da2EyNSH1+LAbK33gQweTJADBdyMwlo5r6PqcP2dzhg="
}
```


```{ "title":"aws kinesis get-records --shard-iterator AAAAAAAAAAHSywljv0zEgPX4NyKdZ5wryMzP9yALs8NeKbUjp1IxtZs1Sp+KEd9I6AJ9ZG4lNR1EMi+9Md/nHvtLyxpfhEzYvkTZ4D9DQVz/mBYWRO6OTZRKnW9gd+efGN2aHFdkH1rJl4BL9Wyrk+ghYG22D2T1Da2EyNSH1+LAbK33gQweTJADBdyMwlo5r6PqcP2dzhg= --region us-east-1 |jq", "language": "json", "tabsToSpaces": 2, "showLineNumbers": true, "highlight": "3-5,7" }
{
  "Records":[ {
    "Data":"dGVzdGRhdGE=",
    "PartitionKey":"123”,
    "ApproximateArrivalTimestamp": 1.441215410867E9,
    "SequenceNumber":"49544985256907370027570885864065577703022652638596431874"
  } ],
  "MillisBehindLatest":24000,
  "NextShardIterator":"AAAAAAAAAAEDOW3ugseWPE4503kqN1yN1UaodY8unE0sYslMUmC6lX9hlig5+t4RtZM0/tALfiI4QGjunVgJvQsjxjh2aLyxaAaPr+LaoENQ7eVs4EdYXgKyThTZGPcca2fVXYJWL3yafv9dsDwsYVedI66dbMZFC8rPMWc797zxQkv4pSKvPOZvrUIudb8UkH3VMzx58Is="
}
```

The data is base64 encoded, so you will have to decode it (using a tool like jq or base64).

## Definitions

Stitching is the process of enhancing streamed data with static data using matching properties.

## Additional resources

* [Genesys Cloud API -All available Analytics Detail Events and properties](https://developer.genesys.cloud/analyticsdatamanagement/analytics/detail/analytics-detail-events)
* [Genesys Cloud API - Wrapup Code Mapping - Useful to get Wrapup Name via Wrapup Code Stitching](https://developer.genesys.cloud/devapps/api-explorer#get-api-v2-outbound-wrapupcodemappings)
* [Genesys Cloud API - Queue Name](https://developer.genesys.cloud/devapps/api-explorer#get-api-v2-routing-queues)